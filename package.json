{
  "name": "discord-ollama",
  "version": "0.1.4",
  "description": "Ollama Integration into discord",
  "main": "build/index.js",
  "exports": "./build/index.js",
  "scripts": {
    "dev-tsx": "tsx watch src/index.ts",
    "dev-mon": "nodemon --config nodemon.json src/index.ts",
    "build": "tsc",
    "prod": "node .",
    "client": "npm i && npm run build && npm run prod",
    "API": "ollama serve",
    "docker:setup": "sudo systemctl start docker",
    "docker:cpu": "docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama-cpu ollama/ollama && docker start ollama-cpu && ollama pull codellama",
    "docker:gpu": "docker run -d --gpus 'device=0' -v ollama:/root/.ollama -p 11434:11434 --name ollama-gpu ollama/ollama && docker start ollama-gpu && ollama pull codellama",
    "docker:clean-cpu": "docker stop ollama-cpu && docker rm ollama-cpu && echo \"Removed ollama-cpu\"",
    "docker:clean-gpu": "docker stop ollama-gpu && docker rm ollama-gpu && echo \"Removed ollama-gpu\"",
    "docker:stop": "sudo systemctl stop docker",
    "docker:clean": "concurrently \"npm:docker:clean-cpu\" \"npm:docker:clean-gpu\" \"npm:docker:stop\"",
    "docker": "npm run docker:setup && npm run docker:gpu",
    "start": "concurrently \"npm:client\""
  },
  "author": "Kevin Dang",
  "license": "ISC",
  "dependencies": {
    "axios": "^1.6.2",
    "concurrently": "^8.2.2",
    "discord.js": "^14.14.1",
    "dotenv": "^16.3.1",
    "ollama": "^0.4.4"
  },
  "devDependencies": {
    "@types/node": "^20.10.5",
    "nodemon": "^3.0.2",
    "ts-node": "^10.9.2",
    "tsx": "^4.6.2",
    "typescript": "^5.3.3"
  },
  "type": "module",
  "engines": {
    "node": ">=16.0.0"
  }
}
